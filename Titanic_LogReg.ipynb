{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORT ALL DATA\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "test_x=test.drop('PassengerId',1) #is this actually working?\n",
    "train_x=train.drop('PassengerId',1)\n",
    "test_x=test.drop('Ticket',1)\n",
    "train_x=train.drop('Ticket',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FIND THE MEDIAN AGE AND REPLACE BLANK VALUES WITH IT\n",
    "median_age=np.median(train_x['Age'].dropna())\n",
    "train_x['Age']=train_x['Age'].fillna(median_age)\n",
    "median_age=np.median(test_x['Age'].dropna())\n",
    "test_x['Age']=test_x['Age'].fillna(median_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SET CABIN TO BINARY FEATURE\n",
    "train_x.loc[train_x['Cabin'].isnull(), 'Cabin'] = 0\n",
    "train_x.loc[train_x['Cabin'] != 0, 'Cabin'] = 1\n",
    "test_x.loc[test_x['Cabin'].isnull(), 'Cabin'] = 0\n",
    "test_x.loc[test_x['Cabin'] != 0, 'Cabin'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SET MISSING EMBARKED VALUES TO THE MOST COMMON ONE\n",
    "train_x['Embarked']=train_x['Embarked'].fillna('S')\n",
    "test_x['Embarked']=test_x['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#USE VALUES INSTEAD OF TEXT IN EMBARKED?\n",
    "train_x['Embarked']=train_x['Embarked'].replace({'S': 0, 'C': 1, 'Q':2})\n",
    "test_x['Embarked']=test_x['Embarked'].replace({'S': 0, 'C': 1, 'Q':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHANGE MISSING FARE VALUES TO THE MEDIAN VALUE\n",
    "test_x['Fare'] = test_x['Fare'].fillna(test_x['Fare'].dropna().median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REPLACE MALE AND FEMALES WITH NUMBERS?\n",
    "train_x['Sex']=train_x['Sex'].replace('male',0)\n",
    "train_x['Sex']=train_x['Sex'].replace('female',1)\n",
    "test_x['Sex']=test_x['Sex'].replace('male',0)\n",
    "test_x['Sex']=test_x['Sex'].replace('female',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEyCAYAAACPj9ldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e9181f2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CONVERT FARE VALUES TO DISCRETE CATEGORIES\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1=fig.add_subplot(1,1,1)\n",
    "values=ax1.hist(train_x['Fare'], bins=7)\n",
    "for i in range(1,len(values[1])):\n",
    "    train_x.loc[(train_x['Fare'] < values[1][i]) & (train_x['Fare'] >= values[1][i-1]),'Fare'] = i-1\n",
    "    test_x.loc[(test_x['Fare'] < values[1][i]) & (test_x['Fare'] >= values[1][i-1]),'Fare'] = i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEyCAYAAACPj9ldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE2xJREFUeJzt3X+MZeV93/H3p4Dxz3jBDGizu+qQZJOYRPVCp4SUqnLA\nifkReYlkKlAUryykTSWs4sZqsqRSE0tFwlJiUkst0iYQryMXm2K7rIAmIfxQ5EqGDHiNF68pG7OF\n8W7ZSQzYrhUa8Ld/3Gfl62XCzM79MQPP+yVd3XOe85z7fO+cu589P+6ZSVUhST37R2tdgCStNYNQ\nUvcMQkndMwgldc8glNQ9g1BS9wxCSd0zCCV1zyCU1L2T17oAgDPOOKNmZ2fXugxJrzOPPPLI31TV\nzHL91kUQzs7OMj8/v9ZlSHqdSfK/V9LPQ2NJ3TMIJXXPIJTUPYNQUvcMQkndMwgldc8glNQ9g1BS\n9wxCSd0zCCV1zyCU1L11ca+xxm92191rXcIrHLrx8rUuQVqSe4SSumcQSureioMwyUlJvpzkrjZ/\ndpKHkjyZ5LNJ3tDaT23zB9vy2cmULknjcSJ7hNcBB4bmPwbcVFVbgeeAa1r7NcBzVfUTwE2tnySt\nWysKwiSbgcuBP2rzAS4C7mhd9gBXtOntbZ62/OLWX5LWpZXuEf4B8JvA99v8O4Dnq+qlNr8AbGrT\nm4BnANryF1r/H5JkZ5L5JPOLi4urLF+SRrdsECb5ZeBoVT0y3LxE11rBsh80VO2uqrmqmpuZWfZP\nCkjSxKzke4QXAu9LchnwRuBHGOwhbkhyctvr2wwcbv0XgC3AQpKTgbcD3xp75ZI0JsvuEVbV9VW1\nuapmgauA+6vqV4EHgPe3bjuAO9v03jZPW35/Vb1ij1CS1otRvkf4W8BvJDnI4BzgLa39FuAdrf03\ngF2jlShJk3VCt9hV1YPAg236G8D5S/T5O+DKMdQmSVPhnSWSumcQSuqeQSipewahpO4ZhJK6ZxBK\n6p5BKKl7BqGk7hmEkrpnEErqnkEoqXsGoaTuGYSSumcQSuqeQSipewahpO4ZhJK6ZxBK6p5BKKl7\nBqGk7hmEkrpnEErqnkEoqXvLBmGSNyZ5OMlXkjye5KOt/ZNJnkqyrz22tfYk+USSg0keS3LepN+E\nJI1iJX/g/UXgoqr6bpJTgC8m+R9t2b+rqjuO638psLU9fg64uT1L0rq07B5hDXy3zZ7SHvUqq2wH\nPtXW+xKwIcnG0UuVpMlY0TnCJCcl2QccBe6tqofaohva4e9NSU5tbZuAZ4ZWX2htx7/mziTzSeYX\nFxdHeAuSNJoVBWFVvVxV24DNwPlJfha4Hvhp4J8BpwO/1bpnqZdY4jV3V9VcVc3NzMysqnhJGocT\numpcVc8DDwKXVNWRdvj7IvDHwPmt2wKwZWi1zcDhMdQqSROxkqvGM0k2tOk3Ae8Bvn7svF+SAFcA\n+9sqe4EPtKvHFwAvVNWRiVQvSWOwkqvGG4E9SU5iEJy3V9VdSe5PMsPgUHgf8K9b/3uAy4CDwPeA\nD46/bEkan2WDsKoeA85dov2if6B/AdeOXpokTYd3lkjqnkEoqXsGoaTuGYSSumcQSuqeQSipewah\npO4ZhJK6ZxBK6p5BKKl7BqGk7hmEkrpnEErqnkEoqXsGoaTuGYSSumcQSuqeQSipewahpO4ZhJK6\nZxBK6p5BKKl7K/kD729M8nCSryR5PMlHW/vZSR5K8mSSzyZ5Q2s/tc0fbMtnJ/sWJGk0K9kjfBG4\nqKreBWwDLklyAfAx4Kaq2go8B1zT+l8DPFdVPwHc1PpJ0rq1bBDWwHfb7CntUcBFwB2tfQ9wRZve\n3uZpyy9OkrFVLEljtqJzhElOSrIPOArcC/w18HxVvdS6LACb2vQm4BmAtvwF4B1LvObOJPNJ5hcX\nF0d7F5I0ghUFYVW9XFXbgM3A+cA7l+rWnpfa+6tXNFTtrqq5qpqbmZlZab2SNHYndNW4qp4HHgQu\nADYkObkt2gwcbtMLwBaAtvztwLfGUawkTcJKrhrPJNnQpt8EvAc4ADwAvL912wHc2ab3tnna8vur\n6hV7hJK0Xpy8fBc2AnuSnMQgOG+vqruSfA34TJL/CHwZuKX1vwX4kyQHGewJXjWBuiVpbJYNwqp6\nDDh3ifZvMDhfeHz73wFXjqU6SZoC7yyR1D2DUFL3DEJJ3TMIJXXPIJTUPYNQUvcMQkndMwgldc8g\nlNQ9g1BS9wxCSd0zCCV1zyCU1D2DUFL3DEJJ3TMIJXXPIJTUPYNQUvcMQkndMwgldc8glNQ9g1BS\n9wxCSd1bNgiTbEnyQJIDSR5Pcl1r/90k30yyrz0uG1rn+iQHkzyR5L2TfAOSNKpl/8A78BLwkap6\nNMnbgEeS3NuW3VRVvzfcOck5wFXAzwA/CvxFkp+sqpfHWbgkjcuye4RVdaSqHm3T3wEOAJteZZXt\nwGeq6sWqego4CJw/jmIlaRJO6BxhklngXOCh1vShJI8luTXJaa1tE/DM0GoLLBGcSXYmmU8yv7i4\neMKFS9K4rDgIk7wV+Bzw4ar6NnAz8OPANuAI8PvHui6xer2ioWp3Vc1V1dzMzMwJFy5J47KiIExy\nCoMQ/HRVfR6gqp6tqper6vvAH/KDw98FYMvQ6puBw+MrWZLGayVXjQPcAhyoqo8PtW8c6vYrwP42\nvRe4KsmpSc4GtgIPj69kSRqvlVw1vhD4NeCrSfa1tt8Grk6yjcFh7yHg1wGq6vEktwNfY3DF+Vqv\nGEtaz5YNwqr6Ikuf97vnVda5AbhhhLokaWq8s0RS9wxCSd0zCCV1zyCU1D2DUFL3DEJJ3TMIJXXP\nIJTUPYNQUvcMQkndMwgldc8glNQ9g1BS9wxCSd0zCCV1zyCU1D2DUFL3DEJJ3TMIJXXPIJTUPYNQ\nUvcMQkndW8kfeN+S5IEkB5I8nuS61n56knuTPNmeT2vtSfKJJAeTPJbkvEm/CUkaxUr2CF8CPlJV\n7wQuAK5Ncg6wC7ivqrYC97V5gEuBre2xE7h57FVL0hgtG4RVdaSqHm3T3wEOAJuA7cCe1m0PcEWb\n3g58qga+BGxIsnHslUvSmJzQOcIks8C5wEPAWVV1BAZhCZzZum0CnhlabaG1SdK6tOIgTPJW4HPA\nh6vq26/WdYm2WuL1diaZTzK/uLi40jIkaexWFIRJTmEQgp+uqs+35mePHfK256OtfQHYMrT6ZuDw\n8a9ZVburaq6q5mZmZlZbvySNbCVXjQPcAhyoqo8PLdoL7GjTO4A7h9o/0K4eXwC8cOwQWpLWo5NX\n0OdC4NeArybZ19p+G7gRuD3JNcDTwJVt2T3AZcBB4HvAB8dasSSN2bJBWFVfZOnzfgAXL9G/gGtH\nrEuSpsY7SyR1zyCU1D2DUFL3DEJJ3TMIJXXPIJTUPYNQUvcMQkndW8mdJVqB2V13r3UJklbJPUJJ\n3TMIJXXPIJTUPYNQUvcMQkndMwgldc8glNQ9g1BS9wxCSd0zCCV1zyCU1D2DUFL3DEJJ3TMIJXVv\n2SBMcmuSo0n2D7X9bpJvJtnXHpcNLbs+ycEkTyR576QKl6RxWcke4SeBS5Zov6mqtrXHPQBJzgGu\nAn6mrfNfkpw0rmIlaRKWDcKq+kvgWyt8ve3AZ6rqxap6CjgInD9CfZI0caOcI/xQksfaofNprW0T\n8MxQn4XW9gpJdiaZTzK/uLg4QhmSNJrVBuHNwI8D24AjwO+39izRt5Z6garaXVVzVTU3MzOzyjIk\naXSrCsKqeraqXq6q7wN/yA8OfxeALUNdNwOHRytRkiZrVUGYZOPQ7K8Ax64o7wWuSnJqkrOBrcDD\no5UoSZO17F+xS3Ib8G7gjCQLwO8A706yjcFh7yHg1wGq6vEktwNfA14Crq2qlydTuiSNx7JBWFVX\nL9F8y6v0vwG4YZSiJGmavLNEUvcMQkndMwgldc8glNS9ZS+WSOMyu+vutS7hhxy68fK1LkHrhHuE\nkrpnEErqnkEoqXsGoaTuGYSSumcQSuqeQSipewahpO4ZhJK6ZxBK6p5BKKl7BqGk7hmEkrpnEErq\nnkEoqXsGoaTuGYSSurdsECa5NcnRJPuH2k5Pcm+SJ9vzaa09ST6R5GCSx5KcN8niJWkcVrJH+Eng\nkuPadgH3VdVW4L42D3ApsLU9dgI3j6dMSZqcZYOwqv4S+NZxzduBPW16D3DFUPunauBLwIYkG8dV\nrCRNwmrPEZ5VVUcA2vOZrX0T8MxQv4XWJknr1rgvlmSJtlqyY7IzyXyS+cXFxTGXIUkrt9ogfPbY\nIW97PtraF4AtQ/02A4eXeoGq2l1Vc1U1NzMzs8oyJGl0qw3CvcCONr0DuHOo/QPt6vEFwAvHDqEl\nab1a9g+8J7kNeDdwRpIF4HeAG4Hbk1wDPA1c2brfA1wGHAS+B3xwAjVL0lgtG4RVdfU/sOjiJfoW\ncO2oRUnSNHlniaTuGYSSumcQSuqeQSipewahpO4ZhJK6ZxBK6p5BKKl7BqGk7hmEkrpnEErqnkEo\nqXsGoaTuGYSSumcQSuresr+PcL2a3XX3Wpeg17j19hk6dOPla11Ct9wjlNQ9g1BS9wxCSd0zCCV1\nzyCU1D2DUFL3DEJJ3Rvpe4RJDgHfAV4GXqqquSSnA58FZoFDwL+qqudGK1OSJmcce4S/UFXbqmqu\nze8C7quqrcB9bV6S1q1JHBpvB/a06T3AFRMYQ5LGZtQgLODPkzySZGdrO6uqjgC05zOXWjHJziTz\nSeYXFxdHLEOSVm/Ue40vrKrDSc4E7k3y9ZWuWFW7gd0Ac3NzNWIdkrRqI+0RVtXh9nwU+AJwPvBs\nko0A7fnoqEVK0iStOgiTvCXJ245NA78E7Af2Ajtatx3AnaMWKUmTNMqh8VnAF5Ice53/WlV/muSv\ngNuTXAM8DVw5epmSNDmrDsKq+gbwriXa/xa4eJSiJGmavLNEUvcMQkndMwgldc8glNQ9g1BS9wxC\nSd0zCCV1zyCU1D2DUFL3Rv3tM5LGZHbX3Wtdwg85dOPla13C1LhHKKl7BqGk7hmEkrpnEErqnkEo\nqXsGoaTuGYSSumcQSuqeQSipewahpO4ZhJK6573Gkpa03u59hsnd/zyxPcIklyR5IsnBJLsmNY4k\njWoiQZjkJOA/A5cC5wBXJzlnEmNJ0qgmtUd4PnCwqr5RVf8P+AywfUJjSdJIJhWEm4BnhuYXWpsk\nrTuTuliSJdrqhzokO4Gdbfa7SZ44gdc/A/ibVdY2qrUc2/Edv+vx87ETHv8fr6TTpIJwAdgyNL8Z\nODzcoap2A7tX8+JJ5qtqbvXlrd5aju34ju/4kxl/UofGfwVsTXJ2kjcAVwF7JzSWJI1kInuEVfVS\nkg8BfwacBNxaVY9PYixJGtXEvlBdVfcA90zo5Vd1SP06GNvxHd/xJyBVtXwvSXod815jSd0zCCV1\n7zUVhNO+fznJrUmOJtk/1HZ6knuTPNmeT5vg+FuSPJDkQJLHk1w3zRqSvDHJw0m+0sb/aGs/O8lD\nbfzPtm8GTESSk5J8OcldazD2oSRfTbIvyXxrm+b235DkjiRfb5+Bn5/itv+p9r6PPb6d5MNTfv//\ntn3u9ie5rX0eJ7L9XzNBuEb3L38SuOS4tl3AfVW1FbivzU/KS8BHquqdwAXAte09T6uGF4GLqupd\nwDbgkiQXAB8DbmrjPwdcM6HxAa4DDgzNT3NsgF+oqm1D312b5vb/T8CfVtVPA+9i8HOYyvhV9UR7\n39uAfwp8D/jCtMZPsgn4N8BcVf0sg2+fXMWktn9VvSYewM8DfzY0fz1w/RTGnQX2D80/AWxs0xuB\nJ6b4M7gT+MW1qAF4M/Ao8HMMvtl/8lLbZcxjbmbwj+0i4C4GdyxNZez2+oeAM45rm8rPHvgR4Cna\nBc21/PwBvwT8zym//2O36Z7O4NstdwHvndT2f83sEbJ+7l8+q6qOALTnM6cxaJJZ4FzgoWnW0A5N\n9wFHgXuBvwaer6qXWpdJboc/AH4T+H6bf8cUx4bBbaF/nuSRdksoTO9n/2PAIvDH7dTAHyV5yxTH\nH3YVcFubnsr4VfVN4PeAp4EjwAvAI0xo+7+WgnDZ+5dfr5K8Ffgc8OGq+vY0x66ql2tweLSZwW8V\neudS3cY9bpJfBo5W1SPDzdMYe8iFVXUeg9Mx1yb5lxMc63gnA+cBN1fVucD/ZbKH4Utq5+DeB/y3\nKY97GoPfWHU28KPAWxhsh+ONZfu/loJw2fuXp+TZJBsB2vPRSQ6W5BQGIfjpqvr8WtQAUFXPAw8y\nOFe5IcmxL+NPajtcCLwvySEGv8btIgZ7iNMYG4CqOtyejzI4P3Y+0/vZLwALVfVQm7+DQTBOe9tf\nCjxaVc+2+WmN/x7gqaparKq/Bz4P/HMmtP1fS0G4Xu5f3gvsaNM7GJy3m4gkAW4BDlTVx6ddQ5KZ\nJBva9JsYfDgPAA8A75/k+FV1fVVtrqpZBtv6/qr61WmMDZDkLUnedmyawXmy/UzpZ19V/wd4JslP\ntaaLga9Na/whV/ODw2KmOP7TwAVJ3tz+HRx7/5PZ/pM+0TrmE6iXAf+LwXmqfz+F8W5jcH7i7xn8\nD30Ng/NU9wFPtufTJzj+v2Cw6/8YsK89LptWDcA/Ab7cxt8P/IfW/mPAw8BBBodMp054O7wbuGua\nY7dxvtIejx/7vE15+28D5tvP/78Dp015/DcDfwu8fahtmuN/FPh6++z9CXDqpLa/t9hJ6t5r6dBY\nkibCIJTUPYNQUvcMQkndMwgldc8glNQ9g1BS9/4/ETNz6OYAv84AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e91b07da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CONVERT FARE VALUES TO DISCRETE CATEGORIES\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax1=fig.add_subplot(1,1,1)\n",
    "values=ax1.hist(train_x['Age'], bins=7)\n",
    "for i in range(1,len(values[1])):\n",
    "    train_x.loc[(train_x['Age'] < values[1][i]) & (train_x['Age'] >= values[1][i-1]),'Age'] = i-1\n",
    "    test_x.loc[(test_x['Age'] < values[1][i]) & (test_x['Age'] >= values[1][i-1]),'Age'] = i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'the Countess' 'Jonkheer']\n",
      "['Mr' 'Mrs' 'Miss' 'Master' 'Ms' 'Col' 'Rev' 'Dr' 'Dona']\n"
     ]
    }
   ],
   "source": [
    "#REDUCE THE NUMBER OF TITLES IN THE DATASET\n",
    "names = list(train_x['Name'])\n",
    "names_test=list(test_x['Name'])\n",
    "\n",
    "titles=[]\n",
    "for name in names:\n",
    "    tmp=name.split(',')\n",
    "    tmp=tmp[1].split('.')\n",
    "    titles.append(tmp[0].strip())\n",
    "    \n",
    "titles_test=[]\n",
    "for name in names_test:\n",
    "    tmp=name.split(',')\n",
    "    tmp=tmp[1].split('.')\n",
    "    titles_test .append(tmp[0].strip()) \n",
    "    \n",
    "train_x['Titles']=titles\n",
    "test_x['Titles']=titles_test\n",
    "\n",
    "print(train_x['Titles'].unique())\n",
    "print(test_x['Titles'].unique())\n",
    "\n",
    "train_x['Titles']=train_x['Titles'].replace({'Mme': 'Mrs', 'Ms': 'Miss', 'Mlle':'Miss',\n",
    "                                              'Don':'Other'\n",
    "                                            , 'Rev':'Other'\n",
    "                                             , 'Dr':'Other'\n",
    "                                             , 'Major':'Other'\n",
    "                                             , 'Lady':'Other'\n",
    "                                             , 'Sir':'Other'\n",
    "                                             , 'Col':'Other'\n",
    "                                             , 'Capt':'Other'\n",
    "                                             , 'the Countess':'Other'\n",
    "                                             , 'Jonkheer':'Other'\n",
    "                                            \n",
    "                                            })\n",
    "\n",
    "\n",
    "test_x['Titles']=test_x['Titles'].replace({'Mme': 'Mrs', 'Ms': 'Miss', 'Mlle':'Miss',\n",
    "                                              'Dona':'Other'\n",
    "                                            , 'Rev':'Other'\n",
    "                                             , 'Dr':'Other'\n",
    "                                             , 'Major':'Other'\n",
    "                                             , 'Lady':'Other'\n",
    "                                             , 'Sir':'Other'\n",
    "                                             , 'Col':'Other'\n",
    "                                             , 'Capt':'Other'\n",
    "                                             , 'the Countess':'Other'\n",
    "                                             , 'Jonkheer':'Other'\n",
    "                                            \n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONE HOT ENCODE THE SEX VALUES\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.preprocessing import  LabelEncoder as LE\n",
    "\n",
    "sex= [['male'], ['female']]\n",
    "enc = LE()\n",
    "enc.fit(train_x['Sex'])\n",
    "new_sex = enc.transform(train_x['Sex'])\n",
    "new_sex = new_sex.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_sex=ohe.fit_transform(new_sex)\n",
    "\n",
    "enc.fit(test_x['Sex'])\n",
    "new_sex_test = enc.transform(test_x['Sex'])\n",
    "new_sex_test = new_sex_test.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_sex_test=ohe.fit_transform(new_sex_test)\n",
    "encoded_sex_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONE HOT ENCODE THE EMBARKED VALUES\n",
    "embarked= [['S'], ['C'],['Q']]\n",
    "enc = LE()\n",
    "enc.fit(train_x['Embarked'])\n",
    "new_embarked = enc.transform(train_x['Embarked'])\n",
    "new_embarked = new_embarked.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_embarked=ohe.fit_transform(new_embarked)\n",
    "\n",
    "enc.fit(test_x['Embarked'])\n",
    "new_embarked_test = enc.transform(test_x['Embarked'])\n",
    "new_embarked_test = new_embarked_test.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_embarked_test=ohe.fit_transform(new_embarked_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONE HOT ENCODE TITLE VALUES\n",
    "Titles= [['Mrs'], ['Mr'],['Miss'],['Other']]\n",
    "enc = LE()\n",
    "enc.fit(train_x['Titles'])\n",
    "new_Titles = enc.transform(train_x['Titles'])\n",
    "new_Titles = new_Titles.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_Titles=ohe.fit_transform(new_Titles)\n",
    "\n",
    "enc.fit(test_x['Titles'])\n",
    "new_Titles_test = enc.transform(test_x['Titles'])\n",
    "new_Titles_test = new_Titles_test.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_Titles_test=ohe.fit_transform(new_Titles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#ONE HOT ENCODE CLASS VALUES\n",
    "Pclass= [[3], [1],[2]]\n",
    "enc = LE()\n",
    "enc.fit(Pclass)\n",
    "new_Pclass = enc.transform(train_x['Pclass'])\n",
    "new_Pclass = new_Pclass.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_Pclass=ohe.fit_transform(new_Pclass)\n",
    "\n",
    "enc.fit(Pclass)\n",
    "new_Pclass_test = enc.transform(test_x['Pclass'])\n",
    "new_Pclass_test = new_Pclass_test.reshape(-1, 1) # Needs to be the correct shape\n",
    "ohe = OHE(sparse=False) #Easier to read\n",
    "encoded_Pclass_test=ohe.fit_transform(new_Pclass_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CREATE A NEW DATAFRAME WITH THE BINNED AND ONE HOT ENCODED VALUES\n",
    "encoded_data=train_x.drop(['Sex','Titles','Embarked','Pclass'],1)\n",
    "encoded_data_test=test_x.drop(['Sex','Titles','Embarked','Pclass'],1)\n",
    "\n",
    "encoded_data['Female']=encoded_sex[:,0]\n",
    "encoded_data['Male']=encoded_sex[:,1]\n",
    "encoded_data['Mrs']=encoded_Titles[:,3]\n",
    "encoded_data['Mr']=encoded_Titles[:,2]\n",
    "encoded_data['Miss']=encoded_Titles[:,1]\n",
    "encoded_data['Other']=encoded_Titles[:,0]\n",
    "encoded_data['C']=encoded_embarked[:,0]\n",
    "encoded_data['Q']=encoded_embarked[:,1]\n",
    "encoded_data['S']=encoded_embarked[:,2]\n",
    "encoded_data['1']=encoded_Pclass[:,0]\n",
    "encoded_data['2']=encoded_Pclass[:,1]\n",
    "encoded_data['3']=encoded_Pclass[:,2]\n",
    "\n",
    "encoded_data_test['Female']=encoded_sex_test[:,0]\n",
    "encoded_data_test['Male']=encoded_sex_test[:,1]\n",
    "encoded_data_test['Mrs']=encoded_Titles_test[:,3]\n",
    "encoded_data_test['Mr']=encoded_Titles_test[:,2]\n",
    "encoded_data_test['Miss']=encoded_Titles_test[:,1]\n",
    "encoded_data_test['Other']=encoded_Titles_test[:,0]\n",
    "encoded_data_test['C']=encoded_embarked_test[:,0]\n",
    "encoded_data_test['Q']=encoded_embarked_test[:,1]\n",
    "encoded_data_test['S']=encoded_embarked_test[:,2]\n",
    "encoded_data_test['1']=encoded_Pclass_test[:,0]\n",
    "encoded_data_test['2']=encoded_Pclass_test[:,1]\n",
    "encoded_data_test['3']=encoded_Pclass_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REMOVE THE PASSENGERID COLUMN\n",
    "encoded_data_test = encoded_data_test.drop('PassengerId',1)\n",
    "encoded_data = encoded_data.drop('PassengerId',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 32 candidates, totalling 256 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:    6.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.828\n",
      "Best parameters set:\n",
      "\tC: 1\n",
      "\tfit_intercept: True\n",
      "\tpenalty: 'l2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 256 out of 256 | elapsed:    8.4s finished\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION WITH GRIDSEARCH\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV as GS\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameters = {'penalty':['l1','l2'],\n",
    "              'fit_intercept':[True,False],\n",
    "              'C':[0.1,1,10,20,50,100,500,1000]}\n",
    "\n",
    "gs = GS(estimator=LogisticRegression(random_state=9999), param_grid=parameters,n_jobs=4,verbose=True,cv=8)\n",
    "gs.fit(encoded_data.drop(['Survived','Name'],1), encoded_data['Survived'])\n",
    "print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gs.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Mrs</th>\n",
       "      <th>Mr</th>\n",
       "      <th>Miss</th>\n",
       "      <th>Other</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018445</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.077097</td>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.543351</td>\n",
       "      <td>0.341994</td>\n",
       "      <td>-0.549199</td>\n",
       "      <td>0.335636</td>\n",
       "      <td>0.085221</td>\n",
       "      <td>-0.149683</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.285904</td>\n",
       "      <td>0.093349</td>\n",
       "      <td>-0.322308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.018445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.101438</td>\n",
       "      <td>-0.077120</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>0.051830</td>\n",
       "      <td>-0.051830</td>\n",
       "      <td>0.055069</td>\n",
       "      <td>0.092554</td>\n",
       "      <td>-0.111789</td>\n",
       "      <td>-0.146515</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>0.188249</td>\n",
       "      <td>-0.010775</td>\n",
       "      <td>-0.153427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>-0.101438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>-0.026599</td>\n",
       "      <td>-0.114631</td>\n",
       "      <td>0.114631</td>\n",
       "      <td>0.061668</td>\n",
       "      <td>-0.250489</td>\n",
       "      <td>0.083462</td>\n",
       "      <td>0.349559</td>\n",
       "      <td>0.068734</td>\n",
       "      <td>-0.059528</td>\n",
       "      <td>-0.026354</td>\n",
       "      <td>-0.054582</td>\n",
       "      <td>-0.055932</td>\n",
       "      <td>0.092548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>-0.077120</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>-0.245489</td>\n",
       "      <td>0.245489</td>\n",
       "      <td>0.223575</td>\n",
       "      <td>-0.333905</td>\n",
       "      <td>0.100998</td>\n",
       "      <td>0.267344</td>\n",
       "      <td>0.060814</td>\n",
       "      <td>-0.011069</td>\n",
       "      <td>-0.081228</td>\n",
       "      <td>-0.017633</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>0.015790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.077097</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>-0.026599</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.021943</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.096812</td>\n",
       "      <td>0.124331</td>\n",
       "      <td>-0.019239</td>\n",
       "      <td>0.111686</td>\n",
       "      <td>-0.031891</td>\n",
       "      <td>-0.070274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.051830</td>\n",
       "      <td>-0.114631</td>\n",
       "      <td>-0.245489</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.550146</td>\n",
       "      <td>0.867334</td>\n",
       "      <td>-0.693916</td>\n",
       "      <td>0.159934</td>\n",
       "      <td>0.119224</td>\n",
       "      <td>-0.082853</td>\n",
       "      <td>-0.074115</td>\n",
       "      <td>-0.098013</td>\n",
       "      <td>-0.064746</td>\n",
       "      <td>0.137143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.051830</td>\n",
       "      <td>0.114631</td>\n",
       "      <td>0.245489</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550146</td>\n",
       "      <td>-0.867334</td>\n",
       "      <td>0.693916</td>\n",
       "      <td>-0.159934</td>\n",
       "      <td>-0.119224</td>\n",
       "      <td>0.082853</td>\n",
       "      <td>0.074115</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>-0.137143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs</th>\n",
       "      <td>0.341994</td>\n",
       "      <td>0.055069</td>\n",
       "      <td>0.061668</td>\n",
       "      <td>0.223575</td>\n",
       "      <td>-0.021943</td>\n",
       "      <td>-0.550146</td>\n",
       "      <td>0.550146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.477160</td>\n",
       "      <td>-0.207749</td>\n",
       "      <td>-0.087987</td>\n",
       "      <td>-0.002550</td>\n",
       "      <td>0.067872</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>0.093608</td>\n",
       "      <td>0.119193</td>\n",
       "      <td>-0.177658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr</th>\n",
       "      <td>-0.549199</td>\n",
       "      <td>0.092554</td>\n",
       "      <td>-0.250489</td>\n",
       "      <td>-0.333905</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.867334</td>\n",
       "      <td>-0.867334</td>\n",
       "      <td>-0.477160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.601857</td>\n",
       "      <td>-0.254903</td>\n",
       "      <td>0.112870</td>\n",
       "      <td>-0.072567</td>\n",
       "      <td>-0.078338</td>\n",
       "      <td>-0.097288</td>\n",
       "      <td>-0.088569</td>\n",
       "      <td>0.155907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss</th>\n",
       "      <td>0.335636</td>\n",
       "      <td>-0.111789</td>\n",
       "      <td>0.083462</td>\n",
       "      <td>0.100998</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>-0.693916</td>\n",
       "      <td>0.693916</td>\n",
       "      <td>-0.207749</td>\n",
       "      <td>-0.601857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.110981</td>\n",
       "      <td>-0.137144</td>\n",
       "      <td>0.036204</td>\n",
       "      <td>0.167531</td>\n",
       "      <td>0.020348</td>\n",
       "      <td>-0.021903</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.085221</td>\n",
       "      <td>-0.146515</td>\n",
       "      <td>0.349559</td>\n",
       "      <td>0.267344</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>0.159934</td>\n",
       "      <td>-0.159934</td>\n",
       "      <td>-0.087987</td>\n",
       "      <td>-0.254903</td>\n",
       "      <td>-0.110981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>-0.084700</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.064918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>-0.149683</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.068734</td>\n",
       "      <td>0.060814</td>\n",
       "      <td>-0.096812</td>\n",
       "      <td>0.119224</td>\n",
       "      <td>-0.119224</td>\n",
       "      <td>-0.002550</td>\n",
       "      <td>0.112870</td>\n",
       "      <td>-0.137144</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.782742</td>\n",
       "      <td>-0.499421</td>\n",
       "      <td>-0.161921</td>\n",
       "      <td>0.189980</td>\n",
       "      <td>-0.015104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>-0.059528</td>\n",
       "      <td>-0.011069</td>\n",
       "      <td>0.124331</td>\n",
       "      <td>-0.082853</td>\n",
       "      <td>0.082853</td>\n",
       "      <td>0.067872</td>\n",
       "      <td>-0.072567</td>\n",
       "      <td>0.036204</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>-0.782742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.148258</td>\n",
       "      <td>0.296423</td>\n",
       "      <td>-0.125416</td>\n",
       "      <td>-0.153329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>-0.026354</td>\n",
       "      <td>-0.081228</td>\n",
       "      <td>-0.019239</td>\n",
       "      <td>-0.074115</td>\n",
       "      <td>0.074115</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>-0.078338</td>\n",
       "      <td>0.167531</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>-0.499421</td>\n",
       "      <td>-0.148258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.155342</td>\n",
       "      <td>-0.127301</td>\n",
       "      <td>0.237449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285904</td>\n",
       "      <td>0.188249</td>\n",
       "      <td>-0.054582</td>\n",
       "      <td>-0.017633</td>\n",
       "      <td>0.111686</td>\n",
       "      <td>-0.098013</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.093608</td>\n",
       "      <td>-0.097288</td>\n",
       "      <td>0.020348</td>\n",
       "      <td>-0.084700</td>\n",
       "      <td>-0.161921</td>\n",
       "      <td>0.296423</td>\n",
       "      <td>-0.155342</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.288585</td>\n",
       "      <td>-0.626738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093349</td>\n",
       "      <td>-0.010775</td>\n",
       "      <td>-0.055932</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>-0.031891</td>\n",
       "      <td>-0.064746</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>0.119193</td>\n",
       "      <td>-0.088569</td>\n",
       "      <td>-0.021903</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.189980</td>\n",
       "      <td>-0.125416</td>\n",
       "      <td>-0.127301</td>\n",
       "      <td>-0.288585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.565210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.322308</td>\n",
       "      <td>-0.153427</td>\n",
       "      <td>0.092548</td>\n",
       "      <td>0.015790</td>\n",
       "      <td>-0.070274</td>\n",
       "      <td>0.137143</td>\n",
       "      <td>-0.137143</td>\n",
       "      <td>-0.177658</td>\n",
       "      <td>0.155907</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.064918</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>-0.153329</td>\n",
       "      <td>0.237449</td>\n",
       "      <td>-0.626738</td>\n",
       "      <td>-0.565210</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Survived       Age     SibSp     Parch      Fare    Female  \\\n",
       "Survived  1.000000  0.018445 -0.035322  0.081629  0.077097 -0.543351   \n",
       "Age       0.018445  1.000000 -0.101438 -0.077120  0.017716  0.051830   \n",
       "SibSp    -0.035322 -0.101438  1.000000  0.414838 -0.026599 -0.114631   \n",
       "Parch     0.081629 -0.077120  0.414838  1.000000 -0.001061 -0.245489   \n",
       "Fare      0.077097  0.017716 -0.026599 -0.001061  1.000000 -0.000818   \n",
       "Female   -0.543351  0.051830 -0.114631 -0.245489 -0.000818  1.000000   \n",
       "Male      0.543351 -0.051830  0.114631  0.245489  0.000818 -1.000000   \n",
       "Mrs       0.341994  0.055069  0.061668  0.223575 -0.021943 -0.550146   \n",
       "Mr       -0.549199  0.092554 -0.250489 -0.333905  0.007403  0.867334   \n",
       "Miss      0.335636 -0.111789  0.083462  0.100998  0.020247 -0.693916   \n",
       "Other     0.085221 -0.146515  0.349559  0.267344 -0.013021  0.159934   \n",
       "C        -0.149683  0.009162  0.068734  0.060814 -0.096812  0.119224   \n",
       "Q         0.168240  0.005389 -0.059528 -0.011069  0.124331 -0.082853   \n",
       "S         0.003650 -0.022060 -0.026354 -0.081228 -0.019239 -0.074115   \n",
       "1         0.285904  0.188249 -0.054582 -0.017633  0.111686 -0.098013   \n",
       "2         0.093349 -0.010775 -0.055932 -0.000734 -0.031891 -0.064746   \n",
       "3        -0.322308 -0.153427  0.092548  0.015790 -0.070274  0.137143   \n",
       "\n",
       "              Male       Mrs        Mr      Miss     Other         C  \\\n",
       "Survived  0.543351  0.341994 -0.549199  0.335636  0.085221 -0.149683   \n",
       "Age      -0.051830  0.055069  0.092554 -0.111789 -0.146515  0.009162   \n",
       "SibSp     0.114631  0.061668 -0.250489  0.083462  0.349559  0.068734   \n",
       "Parch     0.245489  0.223575 -0.333905  0.100998  0.267344  0.060814   \n",
       "Fare      0.000818 -0.021943  0.007403  0.020247 -0.013021 -0.096812   \n",
       "Female   -1.000000 -0.550146  0.867334 -0.693916  0.159934  0.119224   \n",
       "Male      1.000000  0.550146 -0.867334  0.693916 -0.159934 -0.119224   \n",
       "Mrs       0.550146  1.000000 -0.477160 -0.207749 -0.087987 -0.002550   \n",
       "Mr       -0.867334 -0.477160  1.000000 -0.601857 -0.254903  0.112870   \n",
       "Miss      0.693916 -0.207749 -0.601857  1.000000 -0.110981 -0.137144   \n",
       "Other    -0.159934 -0.087987 -0.254903 -0.110981  1.000000  0.024264   \n",
       "C        -0.119224 -0.002550  0.112870 -0.137144  0.024264  1.000000   \n",
       "Q         0.082853  0.067872 -0.072567  0.036204 -0.035225 -0.782742   \n",
       "S         0.074115 -0.090432 -0.078338  0.167531  0.010478 -0.499421   \n",
       "1         0.098013  0.093608 -0.097288  0.020348 -0.084700 -0.161921   \n",
       "2         0.064746  0.119193 -0.088569 -0.021903  0.009903  0.189980   \n",
       "3        -0.137143 -0.177658  0.155907  0.000293  0.064918 -0.015104   \n",
       "\n",
       "                 Q         S         1         2         3  \n",
       "Survived  0.168240  0.003650  0.285904  0.093349 -0.322308  \n",
       "Age       0.005389 -0.022060  0.188249 -0.010775 -0.153427  \n",
       "SibSp    -0.059528 -0.026354 -0.054582 -0.055932  0.092548  \n",
       "Parch    -0.011069 -0.081228 -0.017633 -0.000734  0.015790  \n",
       "Fare      0.124331 -0.019239  0.111686 -0.031891 -0.070274  \n",
       "Female   -0.082853 -0.074115 -0.098013 -0.064746  0.137143  \n",
       "Male      0.082853  0.074115  0.098013  0.064746 -0.137143  \n",
       "Mrs       0.067872 -0.090432  0.093608  0.119193 -0.177658  \n",
       "Mr       -0.072567 -0.078338 -0.097288 -0.088569  0.155907  \n",
       "Miss      0.036204  0.167531  0.020348 -0.021903  0.000293  \n",
       "Other    -0.035225  0.010478 -0.084700  0.009903  0.064918  \n",
       "C        -0.782742 -0.499421 -0.161921  0.189980 -0.015104  \n",
       "Q         1.000000 -0.148258  0.296423 -0.125416 -0.153329  \n",
       "S        -0.148258  1.000000 -0.155342 -0.127301  0.237449  \n",
       "1         0.296423 -0.155342  1.000000 -0.288585 -0.626738  \n",
       "2        -0.125416 -0.127301 -0.288585  1.000000 -0.565210  \n",
       "3        -0.153329  0.237449 -0.626738 -0.565210  1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: 0.828410919036\n",
      "Cross-validated scores without:  Age 0.825032540658\n",
      "Cross-validated scores without:  SibSp 0.823906048906\n",
      "Cross-validated scores without:  Parch 0.817077995203\n",
      "Cross-validated scores without:  Fare 0.828390626828\n",
      "Cross-validated scores without:  Cabin 0.824961426524\n",
      "Cross-validated scores without:  Female 0.828410919036\n",
      "Cross-validated scores without:  Male 0.828410919036\n",
      "Cross-validated scores without:  Mrs 0.828410919036\n",
      "Cross-validated scores without:  Mr 0.82728479291\n",
      "Cross-validated scores without:  Miss 0.828410919036\n",
      "Cross-validated scores without:  Other 0.802467239967\n",
      "Cross-validated scores without:  C 0.828410919036\n",
      "Cross-validated scores without:  Q 0.825012065637\n",
      "Cross-validated scores without:  S 0.828410919036\n",
      "Cross-validated scores without:  1 0.82728479291\n",
      "Cross-validated scores without:  2 0.82728479291\n",
      "Cross-validated scores without:  3 0.829526990464\n"
     ]
    }
   ],
   "source": [
    "#CROSS VALIDATION WITH FEATURE REMOVAL\n",
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l2\")\n",
    "new_c=encoded_data.drop(['Survived','Name'],1)\n",
    "scores = cross_val_score(log_reg, encoded_data.drop(['Survived','Name'],1), encoded_data['Survived'], cv=8)\n",
    "print (\"Cross-validated scores:\", scores.mean())\n",
    "for i in range(0, new_c.shape[1]):\n",
    "    scores = cross_val_score(log_reg, new_c.drop(new_c.columns[i],1), encoded_data['Survived'], cv=8)\n",
    "    print (\"Cross-validated scores without: \",new_c.columns[i], scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: 0.828410919036\n",
      "Cross-validated scores without:  Age 0.826158666784\n",
      "Cross-validated scores without:  SibSp 0.82504222973\n",
      "Cross-validated scores without:  Parch 0.820476848602\n",
      "Cross-validated scores without:  Fare 0.827254263192\n",
      "Cross-validated scores without:  Cabin 0.824961426524\n",
      "Cross-validated scores without:  Male 0.823916103604\n",
      "Cross-validated scores without:  Mrs 0.826138191763\n",
      "Cross-validated scores without:  Mr 0.825052650053\n",
      "Cross-validated scores without:  Miss 0.828410919036\n",
      "Cross-validated scores without:  Other 0.802467239967\n",
      "Cross-validated scores without:  C 0.827254263192\n",
      "Cross-validated scores without:  Q 0.828410919036\n",
      "Cross-validated scores without:  S 0.8272745554\n",
      "Cross-validated scores without:  1 0.828410919036\n",
      "Cross-validated scores without:  2 0.828410919036\n",
      "Cross-validated scores without:  3 0.828410919036\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l2\")\n",
    "new_c=encoded_data.drop(['Survived','Name', \"Female\"],1)\n",
    "scores = cross_val_score(log_reg, encoded_data.drop(['Survived','Name', \"Female\"],1), encoded_data['Survived'], cv=8)\n",
    "print (\"Cross-validated scores:\", scores.mean())\n",
    "for i in range(0, new_c.shape[1]):\n",
    "    scores = cross_val_score(log_reg, new_c.drop(new_c.columns[i],1), encoded_data['Survived'], cv=8)\n",
    "    print (\"Cross-validated scores without: \",new_c.columns[i], scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: 0.8272745554\n",
      "Cross-validated scores without:  Age 0.826158666784\n",
      "Cross-validated scores without:  SibSp 0.82504222973\n",
      "Cross-validated scores without:  Parch 0.82159292003\n",
      "Cross-validated scores without:  Fare 0.827254263192\n",
      "Cross-validated scores without:  Cabin 0.824961426524\n",
      "Cross-validated scores without:  Male 0.821643376331\n",
      "Cross-validated scores without:  Mrs 0.826138191763\n",
      "Cross-validated scores without:  Mr 0.82728479291\n",
      "Cross-validated scores without:  Miss 0.826138191763\n",
      "Cross-validated scores without:  Other 0.802467239967\n",
      "Cross-validated scores without:  C 0.830592605593\n",
      "Cross-validated scores without:  S 0.827233970984\n",
      "Cross-validated scores without:  1 0.823895994208\n",
      "Cross-validated scores without:  2 0.826138191763\n",
      "Cross-validated scores without:  3 0.828410919036\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l2\")\n",
    "new_c=encoded_data.drop(['Survived','Name', \"Female\", \"Q\"],1)\n",
    "scores = cross_val_score(log_reg, encoded_data.drop(['Survived','Name', \"Female\", \"S\"],1), encoded_data['Survived'], cv=8)\n",
    "print (\"Cross-validated scores:\", scores.mean())\n",
    "for i in range(0, new_c.shape[1]):\n",
    "    scores = cross_val_score(log_reg, new_c.drop(new_c.columns[i],1), encoded_data['Survived'], cv=8)\n",
    "    print (\"Cross-validated scores without: \",new_c.columns[i], scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: 0.828410919036\n",
      "Cross-validated scores without:  Age 0.826158666784\n",
      "Cross-validated scores without:  SibSp 0.823926158301\n",
      "Cross-validated scores without:  Parch 0.820476848602\n",
      "Cross-validated scores without:  Fare 0.829506698257\n",
      "Cross-validated scores without:  Cabin 0.827193569381\n",
      "Cross-validated scores without:  Male 0.821643376331\n",
      "Cross-validated scores without:  Mrs 0.82837033462\n",
      "Cross-validated scores without:  Mr 0.828400864338\n",
      "Cross-validated scores without:  Miss 0.825022120335\n",
      "Cross-validated scores without:  Other 0.802467239967\n",
      "Cross-validated scores without:  C 0.829446004446\n",
      "Cross-validated scores without:  S 0.826117899555\n",
      "Cross-validated scores without:  1 0.831728786416\n",
      "Cross-validated scores without:  2 0.822709174272\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l2\")\n",
    "new_c=encoded_data.drop(['Survived','Name', \"Female\", \"Q\", \"3\"],1)\n",
    "scores = cross_val_score(log_reg, encoded_data.drop(['Survived','Name', \"Female\", \"S\",\"2\"],1), encoded_data['Survived'], cv=8)\n",
    "print (\"Cross-validated scores:\", scores.mean())\n",
    "for i in range(0, new_c.shape[1]):\n",
    "    scores = cross_val_score(log_reg, new_c.drop(new_c.columns[i],1), encoded_data['Survived'], cv=8)\n",
    "    print (\"Cross-validated scores without: \",new_c.columns[i], scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: 0.826168904294\n",
      "Cross-validated scores without:  SibSp 0.819411416286\n",
      "Cross-validated scores without:  Parch 0.818265363578\n",
      "Cross-validated scores without:  Fare 0.8228003978\n",
      "Cross-validated scores without:  Cabin 0.822759996197\n",
      "Cross-validated scores without:  Male 0.821653796654\n",
      "Cross-validated scores without:  Mrs 0.827284975722\n",
      "Cross-validated scores without:  Mr 0.822800580613\n",
      "Cross-validated scores without:  Miss 0.826168904294\n",
      "Cross-validated scores without:  Other 0.799078806891\n",
      "Cross-validated scores without:  C 0.826148794899\n",
      "Cross-validated scores without:  Q 0.827264683515\n",
      "Cross-validated scores without:  1 0.82501224845\n",
      "Cross-validated scores without:  3 0.820487268925\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l1\")\n",
    "new_c=encoded_data.drop(['Survived','Name', \"Female\", \"S\", \"2\", \"Age\"],1)\n",
    "scores = cross_val_score(log_reg, encoded_data.drop(['Survived','Name', \"Female\",  \"S\", \"2\", \"Age\"],1), encoded_data['Survived'], cv=8)\n",
    "print (\"Cross-validated scores:\", scores.mean())\n",
    "for i in range(0, new_c.shape[1]):\n",
    "    scores = cross_val_score(log_reg, new_c.drop(new_c.columns[i],1), encoded_data['Survived'], cv=8)\n",
    "    print (\"Cross-validated scores without: \",new_c.columns[i], scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OUTPUT DATA TO FILE\n",
    "import csv\n",
    "import numpy\n",
    "log_reg = LogisticRegression(C=1, fit_intercept=True, penalty=\"l2\")\n",
    "log_reg.fit(encoded_data.drop(['Survived','Name',\"Female\",  \"S\", \"2\", \"Age\", \"Mr\"],1), encoded_data['Survived'])\n",
    "res=log_reg.predict(encoded_data_test.drop(['Name', \"Female\",  \"S\", \"2\", \"Age\", \"Mr\"],1))\n",
    "id = test['PassengerId'].values\n",
    "joined = list(map(list, zip(id,res)))\n",
    "with open(r\"C:\\Users\\Mark\\Documents\\GitHub\\Titanic\\output.csv\", 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL, lineterminator = '\\n')\n",
    "    wr.writerow([\"PassengerId\",\"Survived\"])\n",
    "    for a in joined:\n",
    "        wr.writerow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
